%% LyX 2.0.3 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[english]{article}
\usepackage[T1]{fontenc}
\usepackage[latin9]{inputenc}
\usepackage{color}
\usepackage{amsmath}
\usepackage{esint}

\makeatletter

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% LyX specific LaTeX commands.
%% Because html converters don't know tabularnewline
\providecommand{\tabularnewline}{\\}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.
\usepackage{amsthm}
\usepackage{amsfonts}

\usepackage{algorithm}
\usepackage{algpseudocode}

\makeatother

\usepackage{babel}
\begin{document}

\title{Summary for Hybrid Stochastic Simulation}

\maketitle

\section{Theory}

Here we summarize some thoretical results from the literature about
Hybrid Stochastic Systems and approximation schemes for such systems.
We mostly follow the notation in \cite{crudu2012convergence} and
\cite{kang2013separation}.


\subsection{Description of a biochemical stochastic reaction network}

We assume a stochastic reaction network consisting of
\begin{itemize}
\item $s_{0}$ species $S_{i}$ with copy numbers $x_{i}$, with $i=1,...,s_{0}$
\item $r_{0}$ reactions with corresponding stochiometry vectors $\xi_{k}=\nu'_{k}-\nu{}_{k}$
and reaction rates $\lambda_{k}\left(x\right)$ with $k=1,...,r_{0}$
\end{itemize}
This network can be written as a Markov Jump Process in the random-time-change
representation

\[
X(t)=x(0)+\sum_{k=1}^{r_{0}}Y_{k}\left(\int_{0}^{t}\lambda_{k}\left(X(s)\right)ds\right)\left(\nu'_{k}-\nu_{k}\right)
\]


whereas the $Y_{k}$ are independent unit Poisson processes.

For the following we assume a decomposition $\left(x_{C},x_{D}\right)$
of the $s_{0}$ species, such that $x_{C}$ represents species that
are large, scaling with a parameter $N$, and $x_{D}$ represents
species that are of order $O(1)$. We define $\psi_{i}^{C}=\frac{1}{N}x_{i}^{C}$.

Additionally we partition the $r_{0}$ reactions into $\mathcal{R}_{C},\mathcal{R}_{DC},\mathcal{R}_{D}$,
such that
\begin{itemize}
\item $\mathcal{R}_{C}$ corresponds to reactions that involve only species
from $x_{C}$ and whose reaction rate only depends on $x_{C}$
\item $\mathcal{R}_{D}$ corresponds to reactions that involve only species
from $x_{D}$ and whose reaction rate only depends on $x_{D}$
\item $\mathcal{R}_{DC}$ corresponds to reactions not belonging to $\mathcal{R}_{C}$
or $\mathcal{R}_{D}$
\end{itemize}

\subsection{Convergence of Markov Jump Processes to Piecewise Deterministic Markov
Processes}

\label{sub:PDMP_convergence}

The following theorem states the convergence of a partially scaled
Markov Jump Process into a PDMP (Piecewise Deterministic Markov Process)
\cite{davis1984piecewise}.

For $r\in\mathcal{R}_{C}$ define $\tilde{\lambda}_{r}=\frac{1}{N}\lambda_{r}$
(we can implicitely assume that $\lambda_{r}$ is large and scales
with $N$ for $r\in\mathcal{R}_{C}$ because otherwise these reactions
will not appear in the limit process anyway).

Define $S_{1}\subseteq\mathcal{R}_{DC}$ such that for a reaction
$r\in S_{1}$ the rate $\lambda_{r}$ is large and scales with $N$
and $\xi_{r}^{D}=0$. For each $r\in S_{1}$ define $\tilde{\lambda}_{r}=\frac{1}{N}\lambda_{r}$.

Define $S_{2}\subseteq\mathcal{R}_{DC}$ such that for a reaction
$r\in S_{2}$ the stochiometry $\xi_{r}^{C}$ is large and scales
with $N$. For each $r\in S_{1}$ define $\tilde{\xi}_{r}^{C}=\frac{1}{N}\xi_{r}^{C}$.

Define $S=S_{1}\cup S_{2}$.

Then we can write the random-time-change representation of the stochastic
process corresponding to this reaction network as

\begin{align*}
X^{N}(t)=\left(\Psi_{C}^{N},X_{D}^{N}\right)(t) & =x(0)\\
 & +\sum_{k\in\mathcal{R}_{C}}Y_{k}\left(\int_{0}^{t}\tilde{\lambda}_{k}\left(\Psi_{C}(s)\right)ds\right)\left(\frac{1}{N}\xi{}_{k}^{C},0\right)\\
 & +\sum_{k\in S_{1}}Y_{k}\left(\int_{0}^{t}N\tilde{\lambda}_{k}\left(X(s)\right)ds\right)\left(\frac{1}{N}\xi{}_{k}^{C},0\right)\\
 & +\sum_{k\in S_{2}}Y_{k}\left(\int_{0}^{t}\lambda_{k}\left(X(s)\right)ds\right)\left(\tilde{\xi}_{k}^{C},\xi_{k}^{D}\right)\\
 & +\sum_{k\in\mathcal{R}_{DC\backslash S}}Y_{k}\left(\int_{0}^{t}\lambda_{k}\left(X(s)\right)ds\right)\left(\frac{1}{N}\xi{}_{k}^{C},\xi{}_{k}^{D}\right)\\
 & +\sum_{k\in\mathcal{R}_{D}}Y_{k}\left(\int_{0}^{t}\lambda_{k}\left(X(s)\right)ds\right)\left(0,\xi{}_{k}^{D}\right)
\end{align*}


\newtheorem{theorem}{Theorem}

\begin{theorem}
\label{theo:theorem_convergence}
The limit process of $X^{N}(t)$ for $N\rightarrow\infty$ is
\begin{align*}
	X(t) &
		\equiv\underset{N\rightarrow\infty}{\lim}X^{N}(t) = \left(\Psi_{C}(t),X_{D}(t)\right) \\
	& = x(0) \\
	& + \sum_{k\in\mathcal{R}_{C}}\int_{0}^{t}\tilde{\lambda}_{k}\left(\Psi_{C}(s)\right)ds\left(\xi{}_{k}^{C},0\right) \\
	& + \sum_{k\in S_{1}}\int_{0}^{t}\tilde{\lambda}_{k}\left(X(s)\right)ds\left(\xi{}_{k}^{C},0\right) \\
	& + \sum_{k\in S_{2}}Y_{k}\left(\int_{0}^{t}\lambda_{k}\left(X(s)\right)ds\right)\left(\tilde{\xi}_{k}^{C},\xi_{k}^{D}\right) \\
	& + \sum_{k\in\mathcal{R}_{DC\backslash S}}Y_{k}\left(\int_{0}^{t}\lambda_{k}\left(X(s)\right)ds\right)\left(0,\xi{}_{k}^{D}\right) \\
	& + \sum_{k\in\mathcal{R}_{D}}Y_{k}\left(\int_{0}^{t}\lambda_{k}\left(X(s)\right)ds\right)\left(0,\xi{}_{k}^{D}\right)
\end{align*}
which is equivalent to a PDMP.
\end{theorem}

See \cite{crudu2012convergence} for a rigerous proof.



One should note that the distinction between $\mathcal{R}_{C}$ and
$S_{1}$ is just a formality (the dependence of the $\tilde{\lambda}_{r}$)
so we can define $S_{3}=\mathcal{R}_{C}\cup S_{1}$ and similarly
$S_{4}=\mathcal{R}_{D}\cup(\mathcal{R}_{DC}\backslash S)$ and write
the limit process as

\begin{align*}
X(t) & \equiv\underset{N\rightarrow\infty}{\lim}X^{N}(t)=\left(\Psi_{C}(t),X_{D}(t)\right)\\
 & =x(0)\\
 & +\sum_{k\in S_{3}}\int_{0}^{t}\tilde{\lambda}_{k}\left(X(s)\right)ds\left(\xi{}_{k}^{C},0\right)\\
 & +\sum_{k\in S_{2}}Y_{k}\left(\int_{0}^{t}\lambda_{k}\left(X(s)\right)ds\right)\left(\tilde{\xi}_{k}^{C},\xi_{k}^{D}\right)\\
 & +\sum_{k\in S_{4}}Y_{k}\left(\int_{0}^{t}\lambda_{k}\left(X(s)\right)ds\right)\left(0,\xi{}_{k}^{D}\right)
\end{align*}



\subsection{Averaging of some discrete variables}

\label{sub:PDMP_averaging}

Define $S_{5}\subseteq\mathcal{R}_{DC}$ such that for a reaction
$r\in S_{1}$ the rate $\lambda_{r}$ is large and scales with $N$
and $\xi_{r}^{D}\neq0$. For each $r\in S_{5}$ define $\tilde{\lambda}_{r}=\frac{1}{N}\lambda_{r}$.
Assume a decomposition $\left(x_{D}^{1},x_{D}^{2}\right)$ of the
$x_{D}$ such that $\xi_{r}^{D,1}=0$ and $\xi_{r}^{D,2}\neq0$ ($x_{D}^{1}$
could be empty).

Define $S=S_{1}\cup S_{2}\cup S_{5}$.

Then the random-time-change representation of the stochastic process
can be written as

\begin{align*}
X^{N}(t)=\left(\Psi_{C}^{N},X_{D}^{N}\right)(t) & =x(0)\\
 & +\sum_{k\in\mathcal{R}_{C}}Y_{k}\left(\int_{0}^{t}\tilde{\lambda}_{k}\left(\Psi_{C}(s)\right)ds\right)\left(\frac{1}{N}\xi{}_{k}^{C},0,0\right)\\
 & +\sum_{k\in S_{1}}Y_{k}\left(\int_{0}^{t}N\tilde{\lambda}_{k}\left(X(s)\right)ds\right)\left(\frac{1}{N}\xi{}_{k}^{C},0,0\right)\\
 & +\sum_{k\in S_{5}}Y_{k}\left(\int_{0}^{t}N\tilde{\lambda}_{k}\left(X(s)\right)ds\right)\left(\tilde{\xi}_{k}^{C},0,\xi_{k}^{D,2}\right)\\
 & +\sum_{k\in S_{2}}Y_{k}\left(\int_{0}^{t}\lambda_{k}\left(X(s)\right)ds\right)\left(\tilde{\xi}_{k}^{C},\xi_{k}^{D,1},\xi_{k}^{D,2}\right)\\
 & +\sum_{k\in\mathcal{R}_{DC\backslash S}}Y_{k}\left(\int_{0}^{t}\lambda_{k}\left(X(s)\right)ds\right)\left(\frac{1}{N}\xi{}_{k}^{C},\xi_{k}^{D,1},\xi_{k}^{D,2}\right)\\
 & +\sum_{k\in\mathcal{R}_{D}}Y_{k}\left(\int_{0}^{t}\lambda_{k}\left(X(s)\right)ds\right)\left(0,\xi_{k}^{D,1},\xi_{k}^{D,2}\right)
\end{align*}


If $x_{D}^{2}$ only takes a finite number of values (i.e. there exists
a finite set $K$ such that $X_{D}^{2}\in K$) then the process $X_{D}^{2}$
given $\psi_{C},x_{D}^{1}$ is ergodic (every state can be reached
from a given state).

Define $\bar{\lambda}_{r}\left(\psi_{C},x_{D}^{1}\right)=\int_{K}\tilde{\lambda}_{r}\left(\psi_{C},x_{D}^{1},x_{D}^{2}\right)\nu_{\Psi_{C},x_{D}^{1}}\left(dx_{D}^{2}\right)$
for $r\in S_{5}$ where $\nu_{\Psi_{C},x_{D}^{1}}$ is the unique
invariant measure of the ergodic process $X_{D}^{2}$ given $\psi_{C},x_{D}^{1}$.

\begin{theorem}
\label{theo:theorem_convergence_averaging}
The limit process of $X^{N}(t)$ for $N\rightarrow\infty$ is
\begin{align*}
	X(t) &
		\equiv\underset{N\rightarrow\infty}{\lim}X^{N}(t) = \left(\Psi_{C}(t),X_{D}(t)\right) \\
	& = x(0) \\
	& + \sum_{k\in\mathcal{R}_{C}}\int_{0}^{t}\tilde{\lambda}_{k}\left(\Psi_{C}(s)\right)ds\left(\xi{}_{k}^{C},0,0\right) \\
	& + \sum_{k\in S_{1}}\int_{0}^{t}\tilde{\lambda}_{k}\left(X(s)\right)ds\left(\xi{}_{k}^{C},0,0\right) \\
	& + \sum_{k\in S_{5}}\int_{0}^{t}\bar{\lambda}_{k}\left(X(s)\right)ds\left(\xi{}_{k}^{C},0,0\right) \\
	& + \sum_{k\in S_{2}}Y_{k}\left(\int_{0}^{t}\lambda_{k}\left(X(s)\right)ds\right)\left(\tilde{\xi}_{k}^{C},\xi_{k}^{D,1},\xi_{k}^{D,2}\right) \\
	& + \sum_{k\in\mathcal{R}_{DC\backslash S}}Y_{k}\left(\int_{0}^{t}\lambda_{k}\left(X(s)\right)ds\right)\left(0,\xi_{k}^{D,1},\xi_{k}^{D,2}\right) \\
	& + \sum_{k\in\mathcal{R}_{D}}Y_{k}\left(\int_{0}^{t}\lambda_{k}\left(X(s)\right)ds\right)\left(0,\xi_{k}^{D,1},\xi_{k}^{D,2}\right)
\end{align*}
which is again equivalent to a PDMP.
\end{theorem}


\subsection{Multiscale approximations of stochastic reaction networks}

For the following we limit ourselves to four types of reactions

\medskip{}


\begin{tabular}{|c|c|c|}
\hline 
$\lambda_{k}$ & Reaction & $\nu_{k}$\tabularnewline
\hline 
\hline 
$\kappa'_{k}$ & $\emptyset\rightarrow stuff$ & $0$\tabularnewline
\hline 
$\kappa'_{k}x_{i}$ & $S_{i}\rightarrow stuff$ & $e_{i}$\tabularnewline
\hline 
$\kappa'_{k}V^{-1}x_{i}(x_{i}-1)$ & $2S_{i}\rightarrow stuff$ & $2e_{i}$\tabularnewline
\hline 
$\kappa'_{k}V^{-1}x_{i}x_{j}$ & $S_{i}+S_{j}\rightarrow stuff$ & $e_{i}+e_{j}$\tabularnewline
\hline 
\end{tabular}

\bigskip{}


whereas $V$ corresponds to the volume of the system and the $\kappa'_{k}$
correspond to molecular reaction rates.

Define
\begin{itemize}
\item $z_{i}^{N}(t)=N^{-\alpha_{i}}x_{i}^{N}(t)$ with $\alpha_{i}\geq0$
for $i=1,...,s_{0}$
\item $\kappa_{k}=N^{-\beta_{k}}\kappa'_{k}$ for $k=1,...,r_{0}$
\end{itemize}
with a parameter $N$ that is assumed to be large.

The rates of the four types of reactions can now be written as

\medskip{}


\begin{tabular}{|c|c|}
\hline 
Reaction & $\lambda_{k}^{N}$\tabularnewline
\hline 
\hline 
$\emptyset\rightarrow stuff$ & $N^{\beta_{k}}\kappa_{k}$\tabularnewline
\hline 
$S_{i}\rightarrow stuff$ & $N^{\beta_{k}+\alpha_{i}}\kappa_{k}z_{i}$\tabularnewline
\hline 
$2S_{i}\rightarrow stuff$ & $N^{\beta_{k}+2\alpha_{i}}\kappa_{k}V^{-1}z_{i}(z_{i}-N^{-\alpha_{i}})$\tabularnewline
\hline 
$S_{i}+S_{j}\rightarrow stuff$ & $N^{\beta_{k}+\alpha_{i}+\alpha_{j}}\kappa_{k}V^{-1}z_{i}z_{j}$\tabularnewline
\hline 
\end{tabular}

\bigskip{}


So we have 
\[
\lambda_{k}(x)\left(\nu'_{k}-\nu_{k}\right)=N^{\beta_{k}+\nu_{k}\cdot\alpha}\lambda_{k}^{N}(z)\left(\nu'_{k}-\nu_{k}\right)
\]
 whereas $\alpha=\left(\alpha_{1},...,\alpha_{s_{0}}\right)^{T}$
. With this we can write the corresponding multiscale Markov Jump
Process as

\[
Z_{i}^{N}(t)=z(0)+\sum_{k=1}^{r_{0}}N^{-\alpha_{i}}Y_{k}\left(\int_{0}^{t}N^{\beta_{k}+\nu_{k}\cdot\alpha}\lambda_{k}^{N}\left(Z_{i}^{N}(s)\right)ds\right)\left(\nu'_{ik}-\nu_{ik}\right)
\]


The $\alpha_{i}$ and $\beta_{i}$ should be choosen in such a way,
that the $z_{i}$ and $\kappa_{k}$ are of order 1 in a loose sense.

In addition we introduce a further scaling parameter $\gamma$ for
the time-scale of the process and write

\[
Z_{i}^{N,\gamma}(t)=Z_{i}^{N}(N^{\gamma}t)=z(0)+\sum_{k=1}^{r_{0}}N^{-\alpha_{i}}Y_{k}\left(\int_{0}^{t}N^{\gamma+\beta_{k}+\nu_{k}\cdot\alpha}\lambda_{k}^{N}\left(Z_{i}^{N,\gamma}(s)\right)ds\right)\left(\nu'_{ik}-\nu_{ik}\right)
\]


See \cite{kang2013separation} for a more elaborate discussion.


\subsection{Constraints for selection of the scaling parameters}

\label{sec:multiscale_parametrization_constraints}

In \cite{kang2013separation} Kang et al. derive constraints for the
selection of the scaling parameters $\alpha_{i},\beta_{i},\gamma$
that we recapitulate here.

\newtheorem{condition}{Condition}

\begin{condition}
Define $\Gamma_{i}^{+}=\{k:\nu'_{ik}>\nu_{ik}\}$ and $\Gamma_{i}^{-}=\{k:\nu'_{ik}<\nu_{ik}\}$. Then the condition
\[ 	\underset{k\in\Gamma_{i}^{-}}{\max}\left(\beta_{k}+\nu_{k}\cdot\alpha\right)=\underset{k\in\Gamma_{i}^{+}}{\max}\left(\beta_{k}+\nu_{k}\cdot\alpha\right)
\]
or
\[
	\gamma\leq\alpha_{i}-\underset{k\in\Gamma_{i}^{+}\cup\Gamma_{i}^{-}}{\max}\left(\beta_{k}+\nu_{k}\cdot\alpha\right)
\]
is called the \textbf{species balance condition} and ensures that no species explodes or is driven to zero.
\end{condition}



We can generalize this condition to linear combinations $\Theta\cdot z$
of species with $\Theta\in[0,\infty)^{s_{0}}$.

\begin{condition}
\label{cond:coll_species_balance}
Define $\Gamma_{\Theta}^{+}=\{k:\Theta\cdot\xi_{k}>0\}$ and $\Gamma_{\Theta}^{-}=\{k:\Theta\cdot\xi_{k}<0\}$. Then the condition
\begin{equation}
	\label{cond:coll_species_balance1}
	\underset{k\in\Gamma_{\Theta}^{-}}{\max}\left(\beta_{k}+\nu_{k}\cdot\alpha\right)=\underset{k\in\Gamma_{\Theta}^{+}}{\max}\left(\beta_{k}+\nu_{k}\cdot\alpha\right)
\end{equation}
or
\begin{equation}
	\label{cond:coll_species_balance2} 	\gamma\leq\gamma_{i}\equiv\underset{i:\Theta_{i}>0}{\max}\alpha_{i}-\underset{k\in\Gamma_{\Theta}^{+}\cup\Gamma_{\Theta}^{-}}{\max}\left(\beta_{k}+\nu_{k}\cdot\alpha\right)
\end{equation}
is called the \textbf{collective species balance condition}. It ensures that no linear combination of species explodes or is driven to zero.
\end{condition}



Checking this condition is not trivial but in \cite{kang2013separation}
Kang et al. show a significant simplification to check for the \textbf{collective
species balance condition}:

\newtheorem{lemma}{Lemma}
\label{lemma:coll_species_balance_graph}

\begin{lemma}
Let $G$ be a directed graph where the nodes correspond to the species and a directed edge from $S_{i}\rightarrow S_{j}$ is drawn
if there is a reaction consuming $S_{i}$ and producing $S_{j}$.

Let $G_{j}$ be maximal strongly connected subgraphs such that $G=\cup_{j}G_{j}$ (this decomposition is unique).

Let $\gamma \in \mathbb{R}$ be fixed. If Condition \ref{cond:coll_species_balance} holds for each $\Theta \in [0,\infty)^{s_0}$ with
$\mbox{supp} \left(\Theta\right) \subset G_j$ for all maximal strongly connected subgraphs $G_j$, then Condition \ref{cond:coll_species_balance}
holds for all $\Theta \in [0,\infty)^{s_0}$.
\end{lemma}

This reduces checking Condition \ref{cond:coll_species_balance} for
all $\Theta\in[0,\infty)^{s_{0}}$ to checking Condition \ref{cond:coll_species_balance}
for all $\Theta$ with $\mbox{supp}\left(\Theta\right)\subset G_{j}$
for each of the subgraphs $G_{j}$ separately.

\begin{lemma}
\label{lemma:coll_species_balance_special_case1}
Kang et al. continue to show that Condition \ref{cond:coll_species_balance} also holds for $c_{1}\Theta^{1}+c_{2}\Theta^{2}$ for all $c_{1},c_{2} > 0$ if either

\[
	\mbox{(\ref{cond:coll_species_balance2}) holds for $\Theta^1$ or $\Theta^2$}
\]

or

\begin{align*}
	& \underset{k\in\Gamma_{\Theta^{1}}^{-}}{\max}\left(\beta_{k}+\nu_{k}\cdot\alpha\right)
		= \underset{k\in\Gamma_{\Theta^{1}}^{+}}{\max}\left(\beta_{k}+\nu_{k}\cdot\alpha\right)\nonumber \\
	\neq & \underset{k\in\Gamma_{\Theta^{2}}^{-}}{\max}\left(\beta_{k}+\nu_{k}\cdot\alpha\right)
		= \underset{k\in\Gamma_{\Theta^{2}}^{+}}{\max}\left(\beta_{k}+\nu_{k}\cdot\alpha\right)
\end{align*}
\end{lemma}



\begin{lemma}
\label{lemma:coll_species_balance_special_case2}
If Lemma \ref{lemma:coll_species_balance_special_case1} does not apply but
\[
	\Gamma_{\Theta^{1}}^{+}\cap\Gamma_{\Theta^{2}}^{-} = \emptyset
\]
or
\[
	\Gamma_{\Theta^{1}}^{-}\cap\Gamma_{\Theta^{2}}^{+} = \emptyset
\]
(i.e. the support of $\Theta^{1}$ produces no species that are consumed in the support of $\Theta^{2}$ and vice versa),
then Condition \ref{cond:coll_species_balance} holds for
$c_{1}\Theta^{1}+c_{2}\Theta^{2}$ for all $c_{1},c_{2} > 0$.
\end{lemma}

\begin{lemma}
\label{lemma:coll_species_balance_special_case3}
If Lemma \ref{lemma:coll_species_balance_special_case1} does not apply and
$\Gamma_{\Theta^{1}}^{+}\cap\Gamma_{\Theta^{2}}^{-} \neq \emptyset$
and $\Gamma_{\Theta^{1}}^{-}\cap\Gamma_{\Theta^{2}}^{+} \neq \emptyset$
suppose that Condition \ref{cond:coll_species_balance} also holds for $\Theta^1 - \frac{\Theta^1 \cdot \xi_k}{\Theta^2 \cdot \xi_k} \Theta^2$
for all $k \in \left( \Gamma_{\Theta^1}^+ \cap \Gamma_{\Theta^2}^- \right) \cup \left(\Gamma_{\Theta^1}^- \cap \Gamma_{\Theta^2}^{+} \right)$.
Then Condition \ref{cond:coll_species_balance} also holds for $c_1 \Theta^1 + c_2 \Theta^2$ for all $c_1, c_2 > 0$.
\end{lemma}

The only case that is left now is that

(\ref{cond:coll_species_balance2}) doesn't hold for both $\Theta^{1}$
and $\Theta^{2}$ and $\Gamma_{\Theta^{1}}^{+}\cap\Gamma_{\Theta^{2}}^{-}\neq\emptyset$
and $\Gamma_{\Theta^{1}}^{-}\cap\Gamma_{\Theta^{2}}^{+}\neq\emptyset$
and Condition \ref{cond:coll_species_balance} doesn't hold for $\Theta^{1}-\frac{\Theta^{1}\cdot\xi_{k}}{\Theta^{2}\cdot\xi_{k}}\Theta^{2}$
and

\begin{align*}
 & \underset{k\in\Gamma_{\Theta^{1}}^{-}}{\max}\left(\beta_{k}+\nu_{k}\cdot\alpha\right)=\underset{k\in\Gamma_{\Theta^{1}}^{+}}{\max}\left(\beta_{k}+\nu_{k}\cdot\alpha\right)\\
= & \underset{k\in\Gamma_{\Theta^{2}}^{-}}{\max}\left(\beta_{k}+\nu_{k}\cdot\alpha\right)=\underset{k\in\Gamma_{\Theta^{2}}^{+}}{\max}\left(\beta_{k}+\nu_{k}\cdot\alpha\right)
\end{align*}


all hold. We call this case \emph{non-reducable collective species
balance.} In this case Condition \ref{cond:coll_species_balance}
has to be checked directly (of course this only needs to be done for
all $\Theta\in[0,\infty)^{s_{0}}$ with support in the subgraphs $G_{j}$
with a \emph{non-reducable collective species balance}).

In all other cases it is now sufficient to check Condition \ref{cond:coll_species_balance}
on a set $\Theta^{j,k}$ spanning the subgraph $G_{j}$ for all $j$.

\begin{theorem}
For completeness we state that if
\[ 	\gamma=\underset{i}{\min}\gamma_{i}=\underset{i}{\min}\left(\alpha_{i}-\underset{k\in\Gamma_{\Theta}^{+}\cup\Gamma_{\Theta}^{-}}{\max}\left(\beta_{k}+\nu_{k}\cdot\alpha\right)\right)
\]
then each term
\[
	N^{-\alpha_{i}}Y_{k}\left(\int_{0}^{t}N^{\gamma+\beta_{k}+\nu_{k}\cdot\alpha}\lambda_{k}^{N}\left(Z_{i}^{N,\gamma}(s)\right)ds\right)
\]
converges to
\[
\begin{cases}
	0 &
		\mbox{for }\alpha_{i}>\gamma+\beta_{k}+\nu_{k}\cdot\alpha \\
	Y_{k}\left(\int_{0}^{t}\lambda_{k}^{N}\left(Z_{i}^{N,\gamma}(s)\right)ds\right) &
		\mbox{for }\alpha_{i}=\gamma+\beta_{k}+\nu_{k}\cdot\alpha=0 \\
	\int_{0}^{t}\lambda_{k}^{N}\left(Z_{i}^{N,\gamma}(s)\right)ds &
		\mbox{for }\alpha_{i}=\gamma+\beta_{k}+\nu_{k}\cdot\alpha
\end{cases}
\]
for $N\rightarrow\infty$ which represents a PDMP of the multiscale network.
\end{theorem}




\subsection{Combination of convergence results and multiscale parametrization}

To combine the convergence results from \ref{sub:PDMP_convergence}
with the multiscale Parametrization we note that the membership of
a species $x_{i}$ in $x_{C},x_{D}$ depends on $\alpha_{i}$:

$\begin{cases}
x_{i}\mbox{ belongs to }x_{C} & \mbox{ for }\alpha_{i}>0\\
x_{i}\mbox{ belongs to }x_{D} & \mbox{ for }\alpha_{i}=0
\end{cases}$

We also note that $r\in S_{1}$ if $\gamma+\beta_{r}+\nu_{r}\cdot\alpha>0$
and $\xi_{r}^{D}=0$ (if $\xi_{r}^{D}\neq0$ one can still take the
approach in \ref{sub:PDMP_averaging}).

We can then use Theorem \ref{theo:theorem_convergence} or Theorem
\ref{theo:theorem_convergence_averaging} to derive the PDMP that
corresponds to the limiting process of the multiscale network.


\section{Implementation}

Here we summarize how the adaptive multiscale network is implemented
as a PDMP, how to simulate the PDMP and then go into more details
about the steps of the adaptation.


\subsection{Adaptive Multiscale Network as a PDMP}

To distinguish between stochastic and deterministic reactions we follow
the theoretical foundation stated further up with an additional threshold
parameter $\delta\geq0$

\[
\begin{cases}
Y_{k}\left(\int_{0}^{t}\lambda_{k}^{N}\left(Z_{i}^{N,\gamma}(s)\right)ds\right) & \mbox{for }\alpha_{i}<\delta\\
\int_{0}^{t}\lambda_{k}^{N}\left(Z_{i}^{N,\gamma}(s)\right)ds & \mbox{for }\alpha_{i}\geq\delta
\end{cases}
\]


If any term of a reaction is stochastic by the previous definition,
then the whole reaction is treated stochastically, as we have to simulate
the stochastic event anyway.

The $\alpha_{i}$ and $\beta_{k}$ are computed by solving the following
linear program:

\begin{center}
\begin{equation}
\begin{array}{ccc}
\mbox{maximize } & \sum_{i=1}^{Q}\frac{\alpha_{i}}{A_{i}}+\sum_{i=1}^{W}\frac{\beta_{k}}{B_{k}}\\
\\
\mbox{subject to } & 0\leq\alpha_{i}\leq A_{i} & \mbox{ for each \ensuremath{i\in\{1,...,Q\}}}\\
\mbox{and } & \beta_{k}\leq B_{k} & \mbox{for each }k\in\{1,...,W\}\\
\mbox{and } & \alpha_{i}\geq\beta_{k}+\alpha\cdot\nu_{k} & \mbox{ for each \ensuremath{i\in\{1,...,Q\},\ k\in\{1,...,W\}}}
\end{array}\label{eq:linear_program_alpha_beta}
\end{equation}

\par\end{center}

whereas $A_{i}=\frac{\log(x_{i})}{\log(N)}{\color{green}+1}$ and
$B_{i}=\frac{log(\kappa'_{k})}{\log(N)}{\color{green}+1}$.

If a species $i$ has $\alpha_{i}\geq\delta$ it is a scaled species,
otherwise it is an unscaled species.

In the next step the reaction graph is searched for subnetworks that
can be averaged (see section \ref{subsection:averaging}).


\subsection{Simulation of a PDMP with adaptation}

{\footnotesize \begin{algorithm}
\label{PDMP_algorithm}
\algblock[evolve]{evolve}{until}
\algblockdefx[evolve]{Evolve}{EndEvolve}%
	{\textbf{evolve}}%
	{\textbf{until }}
\begin{algorithmic}
\State $t\gets t_0$
\State $X\gets x_0$
\State $i\gets 0$
\State $Y\gets [\mbox{exprnd}\left(1\right), 0]$
\While {$t_0 \leq t_1$}
	\Evolve
		\State $X$ according to $X'(t) = \sum_{M_D} \lambda_k\left(X(t)\right) \left(\nu'_k - \nu_k\right)$
		\State $Y$ according to $Y'(t) = [0, \sum_{M_S} \lambda_k\left(X(t)\right)]$
	\EndEvolve {either $t=t_1$ or $Y_1 = Y_2$ or \textit{copy number bounds have been crossed}}
	\If {\textit{copy number bounds have been crossed}}
		\State Adapt scaling parameters and update scaled copy numbers
	\ElsIf {$Y_1=Y_2$}
		\State $Y\gets [\mbox{exprnd}\left(1\right), 0]$
		\State $r\gets k \mbox{ with probability } p_k\propto \lambda_k\left(X\right)$
		\State $X\gets X + \nu'_k - \nu_k$
	\EndIf
\EndWhile
\end{algorithmic}
\end{algorithm}}{\footnotesize \par}


\subsection{Copy number bounds}

As stated in the algorithm \ref{PDMP_algorithm} an adaptation procedure
is performed whenever the copy number bounds are crossed. These bounds
are defined by two parameters $\delta\geq0$ and $\eta>0$. The copy
number bounds for an unscaled species $i$ are $0\leq x_{i}<N^{\delta}$
and the bounds for a scaled species $j$ are $N^{-\eta}<z_{j}=N^{-\alpha_{j}}x_{j}<N^{\eta}$.

An unscaled species $i$ that crosses it's upper copy number bound
$N^{\delta}$ can be a scaled species after the adaptation. Only when
the copy number of the scaled species crosses it's lower bound $N^{-\eta}$
will the species revert to an unscaled species. This means that there
is a hysteresis-like effect for the switching between unscaled and
scaled copy numbers for a species. In the range $(N^{-\eta}N^{\delta},N^{\delta})$
of real copy numbers a species can be either scaled or unscaled depending
on whether the species was unscaled or scaled before the switching.
This might seem like an inconvenience of the implementation but actually
this is necessary to prevent endless adaptation and thus a high performance
penalty due to numerical errors and small changes in the copy numbers
and also to enable the detection of crossings of copy number bounds
by root finding during the ODE integration in the first place.


\subsection{Averaging}

\label{subsection:averaging}

After the $\alpha_{i}$ and $\beta_{k}$ have been computed a timescale
$\tau_{i}$ is computed for each species as follows:

\[
\tau_{i}:=\frac{q_{i}}{\sum_{k\ s.t.\ v'_{ik}\neq0}\sum_{j}|q\nu_{jk}|}
\]


whereas $q_{i}=\max(x_{i},1)$.

Now for each possible subset $S$ of species we compute the timescale
of the subset $\tau_{S}$ and the timescale of the outside network
$\tau_{\bar{S}}$ as follows:

\[
\tau_{S}=max_{i\in S}(\tau_{i})
\]


\[
\tau_{\bar{S}}=min_{i\in\bar{S}}(\tau_{i})
\]


whereas $\bar{S}$ is the set of species $j$ that are directly involved
in reactions with species $i\in S$. That is either

\[
\bar{S}=\bar{S}_{1}\cup\bar{S}_{2}
\]


\[
\bar{S}_{1}=\{j:\nu'_{kj}\neq0\mbox{ and }\nu_{ki}\neq0\mbox{ for some }i\in S\mbox{ and }k\}
\]


\[
\bar{S}_{2}=\{j:\nu{}_{kj}\neq0\mbox{ and }\nu'_{ki}\neq0\mbox{ and }l\mbox{ is connected to }j\mbox{ for some }i,l\in S\mbox{ and }k\}
\]


If $\frac{\tau_{\bar{S}}}{\tau_{S}}\geq\theta$ and for all reactions
$k$ there is no $i\in S$ s.t. $\nu_{ki}>1$ and no $i,j\in S$ s.t.
$\nu_{ki}>0$ and $\nu_{kj}>0$ then we remember $S$ as a candidate
subnetwork for averaging.

Finally we start with an empty list $L_{A}$ of species to average
and an empty list $L$ of subnetworks to average and iterate in order
of decreasing number of species over the candidate subnetworks. If
$L_{A}\cap S=\emptyset$ we add all the species from $S$ to $L_{A}$
and add $S$ to $L$. After the iteration $L$ contains all the subnetworks
to average so we treat the copy numbers of all species of those subnetworks
as continuous and all the reactions within those subnetwork as deterministic.

\bibliographystyle{plain}
\bibliography{/home/bhepp/Documents/papers}

\end{document}
